{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b1eae59-777c-43a1-b4dc-811bb343b29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 19:30:04.090247: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 19:30:06.018609: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-21 19:30:06.018635: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-21 19:30:06.627527: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-21 19:30:07.006783: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main Training Function for MutLX\n",
    "\n",
    "This module contains the main code for training and evaluating using MutLX.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc, average_precision_score\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5871725a-175f-4966-9b09-8ec98abc252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import utils_mutLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc40be9b-f684-4c28-ac1a-adb5c1b5acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and TensorFlow can access it.\n"
     ]
    }
   ],
   "source": [
    "# Check if TensorFlow can access the GPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available and TensorFlow can access it.\")\n",
    "else:\n",
    "    print(\"GPU is not available or TensorFlow cannot access it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b6136a-6970-4c9b-a41c-96b357d7c277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac5bc24-8265-4692-90dd-66cb5990b73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--sampling_num SAMPLING_NUM] [--drop_it DROP_IT]\n",
      "                             [--pscore_cf PSCORE_CF] [--auc_cf AUC_CF]\n",
      "                             [--tpr_cf TPR_CF] --input_path INPUT_PATH\n",
      "                             [--out_path OUT_PATH] [--sample_name SAMPLE_NAME]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --input_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekarimi/miniconda3/envs/digipico/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Remove unused imports\n",
    "# import subprocess, linecache, joblib\n",
    "\n",
    "class DropoutPrediction:\n",
    "    \"\"\"Class used to apply dropouts at test time.\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function\n",
    "    def predict(self, x, n_iter=100):\n",
    "        return tf.stack([self.model(x, training=True) for _ in range(n_iter)], axis=0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    nb_classes = 2\n",
    "    cols = range(1, 43)\n",
    "    input_dim = len(cols) - 1\n",
    "\n",
    "    # Argument parsing (unchanged)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch_size', type=int, required=False, default=8, help='The batch size used for training.')\n",
    "    parser.add_argument('--epochs', type=int, required=False, default=10, help='The number of epochs to train.')\n",
    "    parser.add_argument('--sampling_num', type=int, required=False, default=25, help='The number of subsets.')\n",
    "    parser.add_argument('--drop_it', type=int, required=False, default=100, help='The number of predictions sampled by dropping neurons.')\n",
    "    parser.add_argument('--pscore_cf', type=int, required=False, default=0.2, help='Cutoff value for probability scores.')\n",
    "    parser.add_argument('--auc_cf', type=int, required=False, default=0.9, help='Cutoff value for AUC to identify samples with true UTDs.')\n",
    "    parser.add_argument('--tpr_cf', type=int, required=False, default=0.95, help='The required true positive rate for recovery of true UTDs.')\n",
    "    parser.add_argument('--input_path', type=str, required=True, help='Path to CSV file.')\n",
    "    parser.add_argument('--out_path', type=str, required=False, default='.', help='The path under which to store the output.')\n",
    "    parser.add_argument('--sample_name', type=str, required=False, default='DigiPico', help='The name of sample.')\n",
    "    # Parse command line arguments    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load data and normalize\n",
    "    all_set, test_ind, neg_ind, pos_ind, hpos_ind, names = utils_mutLX.prep_typebased(args.input_path, cols)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    test_set = all_set[np.sort(np.concatenate((pos_ind, neg_ind)))]\n",
    "    test_set[:, 1:] = scaler.fit_transform(test_set[:, 1:])\n",
    "    all_set[:, 1:] = scaler.transform(all_set[:, 1:])\n",
    "\n",
    "    out1 = all_set[:, 0]\n",
    "    out2_var = all_set[:, 0]\n",
    "\n",
    "    for cnt in range(args.sampling_num):\n",
    "        # Set tensorboard callback\n",
    "        tbCallBack = keras.callbacks.TensorBoard(log_dir=f\"{args.out_path}/log\")\n",
    "\n",
    "        # Prepare training subset for level 1 training\n",
    "        np.random.seed(cnt+1)\n",
    "        pos_ind_subset = np.random.choice(pos_ind, len(neg_ind), replace=False)\n",
    "        train_set = all_set[np.sort(np.concatenate((pos_ind_subset, neg_ind)))]\n",
    "\n",
    "        # Level 1 training and test\n",
    "        print(f\"Level 1 training: subset {cnt+1}\")\n",
    "        model_L1 = utils_mutLX.build_model(input_dim, nb_classes - 1, type='ml-binary')\n",
    "        history = model_L1.fit(train_set[:, 1:], train_set[:, 0],\n",
    "                               batch_size=args.batch_size,\n",
    "                               epochs=args.epochs,\n",
    "                               verbose=2,\n",
    "                               callbacks=[tbCallBack])\n",
    "        \n",
    "        print(f\"Level 1 test: subset {cnt+1}\")\n",
    "        y_pred = model_L1.predict(test_set[:, 1:])\n",
    "\n",
    "        # Pruning\n",
    "        TPs = [ind for ind, label in enumerate(test_set[:, 0]) if label == 1 and y_pred[ind] > 0.3]\n",
    "        TNs = [ind for ind, label in enumerate(test_set[:, 0]) if label == 0 and y_pred[ind] < 0.7]\n",
    "\n",
    "        # Prepare training subset for level 2 training\n",
    "        np.random.seed(cnt+1)\n",
    "        TPs_subset = np.random.choice(TPs, len(TNs), replace=False)\n",
    "        train_set = test_set[np.sort(np.concatenate((TPs_subset, TNs)))]\n",
    "\n",
    "        # Clear the session to free up memory\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        # Level 2 training and test\n",
    "        print(f\"Level 2 training: subset {cnt+1}\")\n",
    "        model_L2 = utils_mutLX.build_model(input_dim, nb_classes - 1, type='ml-binary')\n",
    "        model_L2.fit(train_set[:, 1:], train_set[:, 0],\n",
    "                     batch_size=args.batch_size,\n",
    "                     epochs=args.epochs,\n",
    "                     verbose=2,\n",
    "                     callbacks=[tbCallBack])\n",
    "        \n",
    "        weights_path = f\"{args.out_path}/{args.sample_name}_logistic_wts.h5\"\n",
    "        model_L2.save_weights(weights_path)\n",
    "\n",
    "        print(f\"Level 2 test: subset {cnt+1}\")\n",
    "\n",
    "        y_pred = model_L2.predict(all_set[:, 1:])\n",
    "        out1 = np.column_stack((out1, y_pred))\n",
    "        \n",
    "        model_T2 = utils_mutLX.build_model(input_dim, nb_classes-1, type='ml-binary-dropout', weights_path=weights_path)\n",
    "        pred_with_dropout = DropoutPrediction(model_T2)\n",
    "        y_pred = pred_with_dropout.predict(all_set[:, 1:], args.drop_it)\n",
    "        y_pred_var = tf.math.reduce_variance(y_pred, axis=0).numpy()\n",
    "        out2_var = np.column_stack((out2_var, y_pred_var))\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    # Calculate final results and save\n",
    "    print(\"Calculate final results\")\n",
    "    y_pred_mean_1 = np.mean(out1[:, 1:], axis=1)\n",
    "    y_pred_mvar = np.mean(out2_var[:, 1:], axis=1)\n",
    "    final = np.column_stack((y_pred_mean_1, y_pred_mvar))\n",
    "\n",
    "    # Plot and calculate thresholds\n",
    "    thr = utils_mutLX.mutLX_plots(final, neg_ind, hpos_ind, args.pscore_cf, args.auc_cf, args.tpr_cf, f\"{args.out_path}/{args.sample_name}\")\n",
    "\n",
    "    final = np.column_stack((final, np.repeat(\"PASS\", len(y_pred_mvar))))\n",
    "    final[final[:,1].astype(float) > thr, 2] = \"FAIL_Uncertain\"\n",
    "    final[final[:,0].astype(float) <= args.pscore_cf, 2] = \"FAIL_LowScore\"\n",
    "    save = np.column_stack((names, final))\n",
    "    header = ['Mutation', 'Type', 'Probability_Score', 'Uncertainty_Score', 'Result']\n",
    "    pd.DataFrame(save.astype(str)).to_csv(f\"{args.out_path}/{args.sample_name}_scores.csv\", header=header, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce0147-cfdf-45b8-9e23-bebadcfd5b77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
